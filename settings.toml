[oobabooga_api]
use_streaming = true
blocking_url = 'http://192.168.1.20:5000/api/v1/generate'
streaming_url = 'ws://192.168.1.20:5005/api/v1/stream'
# preset_name should be a oobabooga preset; None will use the defaults hardcoded into library/ai_requests.py
preset_name = 'None'
context_length = 8192

[prompt_gen]
#prompt_file_path = "processors/few_shot_templates/full_prompt.txt"
prompt_file_path = "user/few_shot_templates/full_prompt.txt"
# Typically, you would generate a prompt for the entire story.
# "continuation" lets you split the story into two parts, a "context" and a "continuation".
# The idea is to prevent later prompts from ignoring previous parts of the story.
continuation_likelyhood = 0.5

[prompt_format]
# Pretty print order of the json prompt files; unexpected keys will be at the end.
json_key_order = ["prompt", "tone", "writing style", "pacing", "point of view", "moment-to-moment detail",
  "sensory detail", "male characters", "female characters", "context", "story"]
# the fields that are preserved when the prompt jsons are converted into the dataset jsons.
allowed_fields = ["prompt", "tone", "writing style", "pacing", "point of view", "moment-to-moment detail",
  "sensory detail", "context", "story", "system suffix"]
# The rate that a tag is left out of prompt in the final dataset.
# The idea is that the less tags there are, the stronger the influence training has on them.
tag_drop_rate = 0.5
# Tags that will be influenced by the tag_drop_rate.
droppable_tags = ['tone', 'writing style', 'pacing', 'sensory detail']
# the % of prompts to go to the validation set instead of the main dataset. 0.0 to 1.0.
validation_set_size = 0.0

[hacks]
# enables tweaks for my particular dataset
enable = true
use_prose_prompt = true
