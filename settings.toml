[oobabooga_api]
request_url = 'http://127.0.0.1:5000/v1/completions'
context_length = 8192
# preset_name should be a oobabooga preset; 'none' will use the defaults hardcoded into library/ai_requests.py
preset_name = 'none'

[prompt_gen]
# A few shot template for generating a prompt.
#   A good template provides 2 or 3 examples.
#   The each example is a series of key value pairs like ">key:value".
#   Successive key-value pairs are grouped together while a newline separates one example from the next.
#   The {story} placeholder will be replaced with the input story chunk.
prompt_file_path = "processors/few_shot_templates/full_prompt.txt"
# continuation_likelyhood: "continuation" lets you split the story into two parts, a "context" and a "continuation".
#   Then, you would generate a prompt for the "continuation" and train on "context + prompt" -> "continuation".
#   The idea is to prevent ignoring previous parts of the story.
continuation_likelyhood = 0.5

[prompt_format]
# json_key_order: Pretty print order of the json files; unexpected keys will be at the end.
json_key_order = ["prompt", "tone", "writing style", "pacing", "point of view", "moment-to-moment detail",
  "sensory detail", "male characters", "female characters", "context", "story"]
# allowed_fields: The fields that used to generate the dataset row.
allowed_fields = ["prompt", "tone", "writing style", "pacing", "point of view", "moment-to-moment detail",
  "sensory detail", "context", "story", "system suffix"]
# replace_unicode_quotes: replaces unicode quotes with ". For example, "story", "context".
replace_unicode_quotes_fields = ["story", "context"]
# tag_drop_rate: The rate that a tag is left out of prompt in the final dataset.
#   The idea is that the less tags there are, the stronger the influence training has on them.
tag_drop_rate = 0.5
# droppable_tags: Tags that will be influenced by the tag_drop_rate.
droppable_tags = ['tone', 'writing style', 'pacing', 'sensory detail']
# validation_set_size: the % of prompts to go to the validation set instead of the main dataset. 0.0 to 1.0.
validation_set_size = 0.1

[randomize_names]
keys_containing_names = ["prompt", "point of view", "male characters", "female characters",
  "context", "story"]
randomization_blacklist = ["The", "N/A", "Unnamed", "Death", "Man", "Woman", "Child", "Saint", "Teacher"]

[hacks]
# enables tweaks for my particular dataset
undo_hyphens_in_prompt = true
redistribute_authors = false
swap_tag_values = false
use_prose_styles = true
